{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb53dc56-9244-4a39-a4d7-2fdf1cf46c97",
   "metadata": {},
   "source": [
    "***Reinforcement Learning using Qlearning and Enhanced Qlearning in a 2 Player Nim's Game***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebea81e9-2b36-4ae4-a794-a333f5a5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game()->list:\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(_st:list)->(int,int):\n",
    "    xored = _st[0] ^ _st[1] ^ _st[2] #so _st is a list of the piles and here we are taking the XOR of them, so this expression only evaluates to 1 if one of the piles has a different num than the other \n",
    "    if xored == 0:\n",
    "        return nim_random(_st) #if all are the same make a random choice\n",
    "    for pile in range(3): #else look at the piles\n",
    "        s = _st[pile] ^ xored # look at the pile and evaluate if it has a different number than 1, if it does s is 1, if it doesn't s is 0\n",
    "        if s <= _st[pile]: # this will hold true if the pile isn't empty.\n",
    "            return _st[pile]-s, pile #set move to 1 less than the pile count from the pile that doesn't have 1 piece left. EX: If a pile has 8 pieces set move to 7\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st:list)->(int,int):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b3818f-e631-4a23-8c62-6c837c7cb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st:list)->(int,int):\n",
    "    global qtable\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15a3c99-6965-4916-9433-5e595fa0febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(_a:str, _b:str):\n",
    "    state, side = init_game(), 'A'  #init_game starts the board which is a list of ints referring to the pile\n",
    "    while True:\n",
    "        engine = Engines[_a] if side == 'A' else Engines[_b] #if it's A turn set engine to the corresponding player\n",
    "        move, pile = engine(state)   #call engine on the state of the board, this calls each individual engine\n",
    "        #print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move  #conduct the move on the pile\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side is the side that performed the last clear above\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides, this is performed at the end of a turn.\n",
    "\n",
    "def play_games(_n:int, _a:str, _b:str)->(int,int): #creates a number _n of games to simulate\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for _ in range(_n):\n",
    "        wins[game(_a, _b)] += 1  #call a game with a and b\n",
    "    # info\n",
    "    print(f\"{_n} games, {_a:>8s}{wins['A']:5d}  {_b:>8s}{wins['B']:5d}\")\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86888fc4-eb1f-4a3b-bb7e-4de585a3731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,     Guru  996    Random    4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(996, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play games\n",
    "#play_games(1000, 'Random', 'Random')\n",
    "play_games(1000, 'Guru', 'Random')\n",
    "#play_games(1000, 'Random', 'Guru')\n",
    "#play_games(1000, 'Guru', 'Guru') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8ee1c1-d3aa-4022-b1ad-a03910eb2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n:int):\n",
    "    global qtable\n",
    "    Reward = 100.0\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32) #30 item array in 11 in 11 in 11\n",
    "    # play _n games\n",
    "    for _ in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1) #random pick\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            \n",
    "            # Switch sides for play and learning\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r:float, _st1:list, move:int, pile:int, q_future_best:float):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2522a028-3e11-48fe-96ef-ec80d5112c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.7 ms, sys: 0 ns, total: 50.7 ms\n",
      "Wall time: 50.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nim_qlearn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56168621-fee0-4240-94f7-33486193fab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  702    Random  298\n",
      "1000 games,   Random  297  Qlearner  703\n",
      "1000 games,   Random  501    Random  499\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner')\n",
    "\n",
    "play_games(1000, 'Random', 'Random') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e98038-8979-4d63-8c70-98a3ac8aaa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 3\n",
      "1000 games, Qlearner  487    Random  513\n",
      "Train : 10\n",
      "1000 games, Qlearner  562    Random  438\n",
      "Train : 100\n",
      "1000 games, Qlearner  658    Random  342\n",
      "Train : 1000\n",
      "1000 games, Qlearner  694    Random  306\n",
      "Train : 10000\n",
      "1000 games, Qlearner  717    Random  283\n",
      "Train : 50000\n",
      "1000 games, Qlearner  740    Random  260\n",
      "Train : 100000\n",
      "1000 games, Qlearner  699    Random  301\n",
      "CPU times: user 7.24 s, sys: 16.4 ms, total: 7.26 s\n",
      "Wall time: 7.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    print(\"Train :\", n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner', 'Random')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4a4f3f-e6e4-4e59-9170-fe80001a7228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.487, 0.562, 0.658, 0.694, 0.717, 0.74, 0.699]\n"
     ]
    }
   ],
   "source": [
    "# Check the ratio of wins wrt to size of the reinforcement model training\n",
    "print(Wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c12418-96ab-421f-a665-057b4602e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn:str):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            print(s, file=fout)\n",
    "\n",
    "qtable_log('qtable_debug.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1b17c-2364-4e87-bd91-9ecf69bac6fc",
   "metadata": {},
   "source": [
    "***Understanding Reinforcement Learning and the Nim's Game***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905bedba-f38a-4b2c-beb5-87b7fad8a7ea",
   "metadata": {},
   "source": [
    "***What is the environment in the Nim model?***\n",
    "\n",
    "The environment in the Nim model is the state of the current board. That is, it's the number of items in each of the piles, by which the agent can impose an action on said environment mainly through removing items from each pile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c13c1a-40cc-4734-8a87-b306cd35a2c0",
   "metadata": {},
   "source": [
    "***What is the agent(s) in the Nim learning model (Hint, not just the Q-learner). Is\n",
    "Guru an agent?***\n",
    "\n",
    "The agent is the player, a game consists of two agents and these players are different algorithms used to act on the environment. Guru is an agent, it's an algorithm that uses a Nim solution to calculate it's next step (the amount of items to take from a pile and which pile to take from). \n",
    "\n",
    "Qlearner is another agent. This one stores the expected value of states and actions in a Q-table, then it uses this table to indicate to the agent the expected reward for taking an action in this particular state. The values of the Q-table are then updated.\n",
    "\n",
    "Random is another agent. This one decides it's action through a random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006ffe0-aaa9-4889-ba4f-55af53b89b31",
   "metadata": {},
   "source": [
    "***What is the reward and penalty in the Nim learning model.***\n",
    "\n",
    "A state in the environment where the player has achieved a winning condition. The reward here is a win, and it's indicated by the current player taking the final item (to clear the last pile).\n",
    "The penalty is being the player that does not win, not taking the item that clears the last pile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb1c84-d819-43ea-8cd4-f2ddfc87fcf1",
   "metadata": {},
   "source": [
    "***How many possible states there could be in the Nim game with a maximum of 10\n",
    "items per pile and 3 piles total?***\n",
    "\n",
    "Consider if there was 3 items per pile, and 2 piles total. There would be in the first pile 4 different item numbers 3, 2, 1, and 0. This is combined with the same number of states in the second pile, thus the number of states would be 16 which is 4^2.\n",
    "\n",
    "Now consider if there was 10 items per pile and 3 piles total, the number of states would be 11^3 = 1331 possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d23083-fbec-4718-b407-901672553fe6",
   "metadata": {},
   "source": [
    "***How many possible unique actions there could be for player 1 take for their first\n",
    "action in a Nim game with 10 items per pile and 3 piles total?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484073d-3ec7-4eea-99c7-99dbc7aa2329",
   "metadata": {},
   "source": [
    "A player can take any amount of chips from one of the piles in its first move. The number of chips they can take is up to the total number of items in any given pile. So they can take up to 10 items which is 10 actions, these actions can occur on one of any of the piles, so 3*10 = ***30 possible initial actions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35697f1-005b-4a98-9bb1-08d8267fc676",
   "metadata": {},
   "source": [
    "***Now we will improve the provided Nim game learning model, by implementing a system of Rewards and Punishments, as well as a system that shuffles player turns during training doing away with the heavy first turn bias. Finally for a good Q learner we must train our Q learner to balance between random learning and exploitation (learning from the Q table itself), otherwise it will not perform well***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa3e2052-2995-46f8-8207-8cd8a9fe354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMS_MX = 10\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st:list)->(int,int):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move\n",
    "\n",
    "def init_game()->list:\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804dabf0-3c30-43a1-9050-251da8606030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "qtable_NEW, Alpha, Gamma, Reward, epsilon = None, 1, 0.8, [100.0,-100.0], 0.2\n",
    "\n",
    "def nim_qlearner_NEW(_st:list)->(int,int):\n",
    "    global qtable_NEW\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable_NEW[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action\n",
    "\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn_NEW(_n:int):\n",
    "    global qtable_NEW\n",
    "    # based on max items per pile\n",
    "    qtable_NEW = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
    "    # play _n games\n",
    "    Reward = [100.0, -100.0]\n",
    "    side = ['A','B']\n",
    "    for _ in range(_n):\n",
    "        side = random.choice(side)\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        count = 0\n",
    "        move = None\n",
    "        pile = None\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            #print('side',side) - debug\n",
    "            #Keep track of last move, pile, and state (coming from previous loop)\n",
    "            lastmove = move\n",
    "            lastpile = pile\n",
    "            if random.random()<epsilon or np.argmax(qtable_NEW[st1[0], st1[1], st1[2]]) == 0:\n",
    "                move, pile = nim_random(st1)\n",
    "                #print('r')\n",
    "                #print(st1, pile, move)\n",
    "            else:\n",
    "                a = np.argmax(qtable_NEW[st1[0], st1[1], st1[2]])  # exploitation\n",
    "                move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "                if move <= 0 or st1[pile] < move:\n",
    "                    move, pile = nim_random(st1) \n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            #print(\"st2\", st2, \"move\", move)\n",
    "            if st2 == [0, 0, 0] and side=='A':  # game ends\n",
    "                qtable_update_NEW(Reward[0], st1, move, pile, 0)  # Reward the one who won\n",
    "                #print(Reward[0], st1, move, pile, 0)\n",
    "                #print(Reward[1], lastState, lastmove, lastpile, 0)  \n",
    "                break  # new game\n",
    "            elif st2 == [0, 0, 0] and side=='B':\n",
    "                qtable_update_NEW(Reward[1], lastState, lastmove, lastpile, 0)# punish the move conducted on the previous state that lead to loss to that player\n",
    "                break\n",
    "            if side=='A':\n",
    "                qtable_update_NEW(0, st1, move, pile, np.max(qtable_NEW[st2[0], st2[1], st2[2]]))\n",
    "            #print(0, st1, move, pile, np.max(qtable_NEW[st2[0], st2[1], st2[2]]))\n",
    "            \n",
    "            \n",
    "            #print(lastmove, lastpile)\n",
    "            # Switch sides for play and learning\n",
    "            lastState = st1 #keep track of the last state\n",
    "            st1 = st2\n",
    "            side = 'B' if side == 'A' else 'A'\n",
    "            \n",
    "            \n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update_NEW(r:float, _st1:list, move:int, pile:int, q_future_best:float):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    #print(_st1[0], _st1[1], _st1[2], qtable[_st1[0], _st1[1], _st1[2], a], 'move', move, 'pile', pile,'best future:', q_future_best, 'reward', r)\n",
    "    qtable_NEW[_st1[0], _st1[1], _st1[2], a] =(1-Alpha)* qtable_NEW[_st1[0], _st1[1], _st1[2], a]+  Alpha * (r + Gamma * q_future_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f8165-ed28-4b9f-afd1-398e716e614f",
   "metadata": {},
   "source": [
    "This algorithm took me a while to create, I tested different methods with two explicit sides, shuffling the player and what not, ultimately the algorithm behaved similarly, and this was the cleanest implementation. Basically, the sides here are implicitly defined, all that matters is a reward associated to the winning move, and a punishment associated to the losing move. This way the Q-table doesn't have to learn just from one side, it could continuously improve and know which game states to avoid and which game states to prioritize via the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1414412d-1d7e-409e-8302-6baf46d9e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner_NEW':nim_qlearner_NEW, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(_a:str, _b:str):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[_a] if side == 'A' else Engines[_b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n:int, _a:str, _b:str)->(int,int):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for _ in range(_n):\n",
    "        wins[game(_a, _b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {_a:>8s}{wins['A']:5d}  {_b:>8s}{wins['B']:5d}\")\n",
    "    return wins['A'], wins['B']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c111d750-30ee-4199-82ff-14026537e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  3\n",
      "1000 games, Qlearner_NEW  501    Random  499\n",
      "Train  10\n",
      "1000 games, Qlearner_NEW  525    Random  475\n",
      "Train  100\n",
      "1000 games, Qlearner_NEW  666    Random  334\n",
      "Train  1000\n",
      "1000 games, Qlearner_NEW  783    Random  217\n",
      "Train  10000\n",
      "1000 games, Qlearner_NEW  803    Random  197\n",
      "Train  50000\n",
      "1000 games, Qlearner_NEW  813    Random  187\n",
      "Train  100000\n",
      "1000 games, Qlearner_NEW  796    Random  204\n",
      "CPU times: user 9.34 s, sys: 13 ms, total: 9.36 s\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn_NEW(n)\n",
    "    print(\"Train \", n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner_NEW', 'Random')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b549e551-71fb-41ab-81bf-d78b5edb235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn:str):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable_NEW[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            print(s, file=fout)\n",
    "\n",
    "qtable_log('qtableNEW_debug.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e077d4a-df65-473f-bec7-67c784aaefed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  479    Random  521\n",
      "1000 games, Qlearner  505    Random  495\n",
      "1000 games, Qlearner  643    Random  357\n",
      "1000 games, Qlearner  728    Random  272\n",
      "1000 games, Qlearner  728    Random  272\n",
      "1000 games, Qlearner  709    Random  291\n",
      "1000 games, Qlearner  727    Random  273\n",
      "CPU times: user 7.24 s, sys: 12.1 ms, total: 7.25 s\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner', 'Random')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca29a84-bfff-443f-af01-885ced7ed910",
   "metadata": {},
   "source": [
    "In comparison to the Random learner, Qlearner_NEW overcomes Qlearner when faced off against Random. With increased training size, the Qlearner_NEW is able to achieve a higher win rate, acquiring increased learning capability from the presence of a penalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d80ca2c-a4f9-46f3-86e3-06c1b1681c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  529  Qlearner_NEW  471\n",
      "1000 games, Qlearner  505  Qlearner_NEW  495\n",
      "1000 games, Qlearner  510  Qlearner_NEW  490\n",
      "1000 games, Qlearner  302  Qlearner_NEW  698\n",
      "1000 games, Qlearner  256  Qlearner_NEW  744\n",
      "1000 games, Qlearner  297  Qlearner_NEW  703\n",
      "1000 games, Qlearner  318  Qlearner_NEW  682\n",
      "CPU times: user 16.5 s, sys: 8.15 ms, total: 16.5 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    nim_qlearn_NEW(n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner', 'Qlearner_NEW')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5257da18-1fb2-40a8-8506-01a469ee2d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner_NEW  532  Qlearner  468\n",
      "1000 games, Qlearner_NEW  453  Qlearner  547\n",
      "1000 games, Qlearner_NEW  475  Qlearner  525\n",
      "1000 games, Qlearner_NEW  718  Qlearner  282\n",
      "1000 games, Qlearner_NEW  792  Qlearner  208\n",
      "1000 games, Qlearner_NEW  731  Qlearner  269\n",
      "1000 games, Qlearner_NEW  760  Qlearner  240\n",
      "CPU times: user 15.7 s, sys: 384 ms, total: 16.1 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    nim_qlearn_NEW(n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner_NEW', 'Qlearner')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720937cf-2f2c-4ad5-8f8b-1e9e4c1e0e9c",
   "metadata": {},
   "source": [
    "When faced off against Q_learner, Qlearner_NEW beats Qlearner when learned on a larger sample size, this indicates that the capacity for learning information for the new algorithm is greater than the Qlearner. Qlearner when trained on samples of 100, and 1000, is able to comfortably overcome the first turn bias which is present all throughout the increased training sizes. This indicates that the addition of the penalty gives more information per training turn.\n",
    "\n",
    "There is a heavy first turn bias when both algorithms are pitted against each other and I'm unsure why this is. I've tried with this implementation to not give the computer the first turn, in hopes that the first turn bias can be remedied. What helped is actually inserting a exploitation/exploration method in my training protocol, while maintaining proper sides shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc50f050-359b-4b5b-99ea-f80e92850d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner_NEW  717  Qlearner  283\n",
      "1000 games, Qlearner  255  Qlearner_NEW  745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(255, 745)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(1000, 'Qlearner_NEW', 'Qlearner')\n",
    "play_games(1000, 'Qlearner', 'Qlearner_NEW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "575e73d5-4582-46ab-b16a-c2db7da02a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner_NEW  794    Random  206\n",
      "1000 games,   Random  233  Qlearner_NEW  767\n",
      "1000 games, Qlearner  717    Random  283\n",
      "1000 games,   Random  271  Qlearner  729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(271, 729)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(1000, 'Qlearner_NEW', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner_NEW')\n",
    "\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6458e330-070b-4f53-87ac-eece2a3f9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner_NEW   10      Guru  990\n",
      "1000 games,     Guru  997  Qlearner_NEW    3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(997, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(1000, 'Qlearner_NEW', 'Guru')\n",
    "play_games(1000, 'Guru', 'Qlearner_NEW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aaa88807-1de7-453a-a05e-945502097606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner   24      Guru  976\n",
      "1000 games,     Guru  997  Qlearner    3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(997, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(1000, 'Qlearner', 'Guru')\n",
    "play_games(1000, 'Guru', 'Qlearner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201fdfb0-5af5-4a70-8f4e-b61ef0770424",
   "metadata": {},
   "source": [
    "Q_learner_New has no improvement when compared against Guru. Guru exploits the board, basically taking the maximal amount of items in a pile save 1, then doing this across all piles, by the time it comes to the last piles they move then the opponent moves and then they clear the board. To be able to do this we need to take the best action every state action that we get, and currently the q-learning doesn't implement a method to solve the Nim game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7d78c-49c8-4879-a992-3c1d6461100b",
   "metadata": {},
   "source": [
    "Essentially, the Q_learner learning against both Random and itself, both pale in comparison to a solver like Guru. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
