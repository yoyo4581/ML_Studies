{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de514149-3463-4105-a80b-3e24ef0a953b",
   "metadata": {},
   "source": [
    "# Training a Pytorch CNN on the Intel Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b7469e-c0cf-4b1d-931b-db1aefd90867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version= 2.1.1\n",
      "torchaudio version= 2.1.1\n",
      "torchvision version= 0.16.1\n",
      "CUDA available= False\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as atransforms\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as vtransforms\n",
    "\n",
    "# Set the GPU to device 0\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "print(f'PyTorch version= {torch.__version__}')\n",
    "print(f'torchaudio version= {torchaudio.__version__}')\n",
    "print(f'torchvision version= {torchvision.__version__}')\n",
    "print(f'CUDA available= {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a6c659-2852-47ed-86ed-47e6b2c5152a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # CUDA Installation\n",
    "    print('CUDA Version')\n",
    "    !nvcc --version\n",
    "    print()\n",
    "\n",
    "    # CUDNN Installation\n",
    "    print(f'CUDNN Version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Number of CUDA Devices: {torch.cuda.device_count()}')\n",
    "    print(f'Active CUDA Device: {torch.cuda.current_device()}')\n",
    "    print(f'Available devices: {torch.cuda.device_count()}, Name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Current CUDA device: {torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f668e-7256-42da-9be2-62b0ac38b91b",
   "metadata": {},
   "source": [
    "Notes\n",
    "1. train data and test data in torch data loaders. Uses image size and batch size set at 28 and 5000 to compose a vtransforms which uses the image data, resizes it to the image size 28. Mnist image size is 28x28 already so this is just a confirmation. Turns the image data into tensor object and normalize it. Sets it into dataloader object after shuffling the data.\n",
    "2. Fit on training data with info parameter set to true. Info is just to output so viewer can see progress. Initialize layers and send the out_class so the last layer has the same size as the number of output classes. Layers occur in sequential order:\n",
    "    1. Conv2D passes the number of img_channels, number of neurons in the hidden layer, and the kernel size=5.\n",
    "    2. ReLu activation layer\n",
    "    3. Max Pool\n",
    "    4. Batch Normalization in 2d\n",
    "    5. Flatten\n",
    "    6. Linear layer with 16(hidden layer size)*2*4*4, 1024 linear size, 512 in features, 1024 out features.\n",
    "    7. Batch Normalization in 1d, 1024.\n",
    "    8. Linear layer with 1024 features and 10 output features (classes)\n",
    "3. Conduct through epochs, the optimizer is a Rprop:\n",
    "    1. Optimizer zero_grad\n",
    "    2. Training\n",
    "    3. Loss_function\n",
    "    4. Backpropogation\n",
    "    5. Optimization step.\n",
    "\n",
    "Let's do similar to the intel data imageset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bf865-cbea-4656-9bdc-946114a94c54",
   "metadata": {},
   "source": [
    "**Loading training, testing, and validation data as a data loader object with modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b162b2f-60cd-4cc5-b5e5-e474fa54d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "BATCH_SIZE=5000\n",
    "\n",
    "transforms = transforms.Compose([transforms.Resize((128,128)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ]) # could be augmentation\n",
    "\n",
    "train_dir = '/home/yahya/Downloads/archive (4)/seg_train/seg_train/'\n",
    "test_dir = '/home/yahya/Downloads/archive (4)/seg_test/seg_test/'\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = BATCH_SIZE)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626809f2-872c-405d-baea-de781d2fc6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(validation_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a52758-c5b4-4231-a50a-e99e6a97b07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "IMG_SIZE = 128\n",
    "IMG_CHANNEL= 3  # color channel\n",
    "\n",
    "MLP_HIDDEN= 16  # Hidden layer size\n",
    "\n",
    "LAST_LINEAR_SIZE = 1024  # 1024 is arbitrary\n",
    "N_CLASSES = 6  # output layer size\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "class PyTorchCNN(torch.nn.Module):\n",
    "    def __init__(self, epochs=10, eta=0.001, batch_size=1000, seed=0):\n",
    "        super(PyTorchCNN, self).__init__()\n",
    "        self.random = np.random.RandomState(seed)  # shuffle mini batches\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.batch_size = batch_size  # size of training batch - 1 would not work\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.model = None\n",
    "\n",
    "    def init_layers(self, _K):\n",
    "        import torch.nn as nn\n",
    "        self.model = nn.Sequential(\n",
    "            # Conv -> (?, IMG_SIZE, IMG_SIZE, MLP_HIDDEN)\n",
    "            # Pool -> (?, IMG_SIZE/2, IMG_SIZE/2, MLP_HIDDEN)\n",
    "            nn.Conv2d(IMG_CHANNEL, MLP_HIDDEN, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.BatchNorm2d(MLP_HIDDEN),\n",
    "\n",
    "            nn.Conv2d(MLP_HIDDEN, MLP_HIDDEN*2, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(start_dim=1),\n",
    "            \n",
    "            # 4*4 is computed in the above cell - findConv2dOutShape\n",
    "            nn.Linear(MLP_HIDDEN*2 * 29*29, LAST_LINEAR_SIZE), #27k to 1k\n",
    "            #nn.BatchNorm1d(LAST_LINEAR_SIZE),\n",
    "            nn.Linear(LAST_LINEAR_SIZE, _K) #1k to 6\n",
    "        )\n",
    "\n",
    "    def predict(self, _X):\n",
    "        _X = torch.FloatTensor(_X)\n",
    "        assert self.model is not None\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(_X)\n",
    "        ypred = torch.argmax(logits, dim=1)\n",
    "        return ypred.cpu().numpy()\n",
    "\n",
    "    def fit(self, _train_dl, _valid_dl, info=False):\n",
    "        import sys\n",
    "        self.init_layers(N_CLASSES)\n",
    "\n",
    "        optimizer = torch.optim.Rprop(self.model.parameters(), lr=self.eta)\n",
    "\n",
    "        # The main training loop\n",
    "        for e in range(self.epochs):\n",
    "            for data in _train_dl:\n",
    "                X, y = data[0], data[1]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                net_out = self.model(X)\n",
    "                loss = self.loss_func(net_out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            for vdata in _valid_dl:\n",
    "                X_val, y_val = vdata[0], vdata[1]\n",
    "                continue\n",
    "                \n",
    "            if info:\n",
    "                with torch.no_grad():\n",
    "                    acc = accuracy_score(data[1].tolist(), np.argmax(self.model(X).cpu(), axis=1).tolist())\n",
    "                    val_acc = accuracy_score(vdata[1].tolist(), np.argmax(self.model(X_val).cpu(), axis=1).tolist())\n",
    "                print(f\"\\r{e+1:02d}/{self.epochs:02d} | Loss: {loss:<6.2f} | Train/Valid Acc.: {acc*100:.2f}%/{val_acc*100:.2f}%\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b8e8d7a-bead-40c9-adce-6b6317d4a187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchCNN(\n",
      "  (loss_func): CrossEntropyLoss()\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=26912, out_features=1024, bias=True)\n",
      "    (8): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = PyTorchCNN(epochs=10, eta=0.001, batch_size=BATCH_SIZE)\n",
    "\n",
    "# example - the fit function will override the NN configuration\n",
    "cnn.init_layers(N_CLASSES)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30df9bec-e29f-46b1-9cf0-832400ba380e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 62)\n",
      "(29, 29)\n"
     ]
    }
   ],
   "source": [
    "# Compute H, W for image size 28 - see the nn.Sequential below\n",
    "def findConv2dOutShape(_H, _W, _conv, _pool=2):\n",
    "    kernel_size, stride, padding, dilation = _conv.kernel_size, _conv.stride, _conv.padding, _conv.dilation\n",
    "    H = np.floor((_H+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    W = np.floor((_W+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "    if _pool:\n",
    "        H, W = H/_pool, W/_pool\n",
    "    return int(H), int(W)\n",
    "\n",
    "print(findConv2dOutShape(128, 128, torch.nn.Conv2d(3, 16, 5))) #image size 28, uses a conv2d layer in-channels:1 outchannels:16, 5 kernel size, stride:1, no-padding.\n",
    "print(findConv2dOutShape(62, 62, torch.nn.Conv2d(32, 64, 5))) #image size 12, uses a conv2d layer in-channels:16, outchannels:32, 5 kernel size, stride:1, no-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff98447f-14aa-443c-b328-e889e7be1045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/10 | Loss: 8.05   | Train/Valid Acc.: 26.63%/24.13%\n",
      "02/10 | Loss: 2.57   | Train/Valid Acc.: 34.45%/33.64%\n",
      "03/10 | Loss: 1.39   | Train/Valid Acc.: 44.87%/46.65%\n",
      "04/10 | Loss: 1.18   | Train/Valid Acc.: 56.92%/51.75%\n",
      "05/10 | Loss: 1.05   | Train/Valid Acc.: 57.65%/53.46%\n",
      "06/10 | Loss: 0.98   | Train/Valid Acc.: 66.21%/60.98%\n",
      "07/10 | Loss: 0.93   | Train/Valid Acc.: 66.69%/63.90%\n",
      "08/10 | Loss: 0.93   | Train/Valid Acc.: 66.04%/65.79%\n",
      "09/10 | Loss: 0.88   | Train/Valid Acc.: 69.63%/66.82%\n",
      "10/10 | Loss: 0.79   | Train/Valid Acc.: 72.23%/67.75%\n",
      "CPU times: user 38min 52s, sys: 8min 59s, total: 47min 51s\n",
      "Wall time: 12min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn.fit(train_loader, validation_loader, info=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b241c3-ffa3-4a1d-b384-ba7abd0d670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6876666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test performance\n",
    "y_test, y_pred = [], []\n",
    "for data in test_loader:\n",
    "    X = data[0]\n",
    "    y_test += data[1].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred += cnn.predict(X).tolist()\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf07f8-4d69-4246-85b4-54929878eead",
   "metadata": {},
   "source": [
    "### Adding dropout layers to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a77c3a-af0e-46de-9c50-e592cf5b2348",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add regularization and/or drop-out features to your CNN. Report your model's best\n",
    "performance. As the performance standard deviation decreases the model is deemed to be\n",
    "more robust. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d1560d-0977-4fa8-8045-eba2ec0cb9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "IMG_SIZE = 128\n",
    "IMG_CHANNEL= 3  # color channel\n",
    "\n",
    "MLP_HIDDEN= 16  # Hidden layer size\n",
    "\n",
    "LAST_LINEAR_SIZE = 1024  # 1024 is arbitrary\n",
    "N_CLASSES = 6  # output layer size\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "class PyTorchCNN(torch.nn.Module):\n",
    "    def __init__(self, epochs=10, eta=0.001, batch_size=1000, seed=0):\n",
    "        super(PyTorchCNN, self).__init__()\n",
    "        self.random = np.random.RandomState(seed)  # shuffle mini batches\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.batch_size = batch_size  # size of training batch - 1 would not work\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.model = None\n",
    "\n",
    "    def init_layers(self, _K):\n",
    "        import torch.nn as nn\n",
    "        self.model = nn.Sequential(\n",
    "            # Conv -> (?, IMG_SIZE, IMG_SIZE, MLP_HIDDEN)\n",
    "            # Pool -> (?, IMG_SIZE/2, IMG_SIZE/2, MLP_HIDDEN)\n",
    "            nn.Conv2d(IMG_CHANNEL, MLP_HIDDEN, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.BatchNorm2d(MLP_HIDDEN),\n",
    "\n",
    "            nn.Conv2d(MLP_HIDDEN, MLP_HIDDEN*2, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(start_dim=1),\n",
    "            \n",
    "            nn.Dropout(0.5, inplace=True),\n",
    "            # 4*4 is computed in the above cell - findConv2dOutShape\n",
    "            nn.Linear(MLP_HIDDEN*2 * 29*29, LAST_LINEAR_SIZE), #27k to 10k\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.BatchNorm1d(LAST_LINEAR_SIZE),\n",
    "            nn.Dropout(0.5, inplace=True),\n",
    "            nn.Linear(LAST_LINEAR_SIZE, _K) #1k to 6\n",
    "        )\n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()\n",
    "\n",
    "    def predict(self, _X):\n",
    "        _X = torch.FloatTensor(_X)\n",
    "        assert self.model is not None\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(_X)\n",
    "        ypred = torch.argmax(logits, dim=1)\n",
    "        return ypred.cpu().numpy()\n",
    "\n",
    "    def fit(self, _train_dl, _valid_dl, info=False):\n",
    "        import sys\n",
    "        import torch.nn as nn\n",
    "        self.init_layers(N_CLASSES)\n",
    "\n",
    "        optimizer = torch.optim.Rprop(self.model.parameters(), lr=self.eta) #this optimizer does not support regularization, instead it uses resilient back-propagation.\n",
    "\n",
    "        # The main training loop\n",
    "        for e in range(self.epochs):\n",
    "            for data in _train_dl:\n",
    "                X, y = data[0], data[1]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                net_out = self.model(X)\n",
    "                loss = self.loss_func(net_out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            for vdata in _valid_dl:\n",
    "                X_val, y_val = vdata[0], vdata[1]\n",
    "                continue\n",
    "                \n",
    "            if info:\n",
    "                with torch.no_grad():\n",
    "                    acc = accuracy_score(data[1].tolist(), np.argmax(self.model(X).cpu(), axis=1).tolist())\n",
    "                    val_acc = accuracy_score(vdata[1].tolist(), np.argmax(self.model(X_val).cpu(), axis=1).tolist())\n",
    "                print(f\"\\r{e+1:02d}/{self.epochs:02d} | Loss: {loss:<6.2f} | Train/Valid Acc.: {acc*100:.2f}%/{val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a698d10-5767-4808-b546-7f41bf223f85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchCNN(\n",
      "  (loss_func): CrossEntropyLoss()\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Dropout(p=0.5, inplace=True)\n",
      "    (8): Linear(in_features=26912, out_features=1024, bias=True)\n",
      "    (9): Dropout(p=0.5, inplace=True)\n",
      "    (10): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = PyTorchCNN(epochs=10, eta=0.001, batch_size=BATCH_SIZE)\n",
    "\n",
    "# example - the fit function will override the NN configuration\n",
    "cnn.init_layers(N_CLASSES)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c848011e-9cdd-44f5-9c7b-6f912c87cbef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/10 | Loss: 6.27   | Train/Valid Acc.: 37.21%/35.53%\n",
      "02/10 | Loss: 1.63   | Train/Valid Acc.: 34.85%/32.29%\n",
      "03/10 | Loss: 1.34   | Train/Valid Acc.: 47.72%/47.18%\n",
      "04/10 | Loss: 1.18   | Train/Valid Acc.: 57.65%/55.81%\n",
      "05/10 | Loss: 1.04   | Train/Valid Acc.: 61.73%/59.48%\n",
      "06/10 | Loss: 1.03   | Train/Valid Acc.: 64.33%/61.26%\n",
      "07/10 | Loss: 0.97   | Train/Valid Acc.: 64.82%/63.19%\n",
      "08/10 | Loss: 0.88   | Train/Valid Acc.: 66.86%/64.43%\n",
      "09/10 | Loss: 0.88   | Train/Valid Acc.: 66.45%/65.86%\n",
      "10/10 | Loss: 0.84   | Train/Valid Acc.: 69.71%/67.14%\n",
      "CPU times: user 39min, sys: 9min 21s, total: 48min 21s\n",
      "Wall time: 12min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn.fit(train_loader, validation_loader, info=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c2cd013-770b-4b2d-a1e3-eca488789ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test performance\n",
    "y_test, y_pred = [], []\n",
    "for data in test_loader:\n",
    "    X = data[0]\n",
    "    y_test += data[1].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred += cnn.predict(X).tolist()\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0d3d1-6e65-47ab-9b7c-04123cf56742",
   "metadata": {},
   "source": [
    "Adding dropout helped the accuracy marginally, and the standard deviation in the reclassification accuracy was greatly helped across training runs. This could be improved by adding more dropout, however a 2% difference between training and reclassification accuracy is pretty neglegible to be honest. Our model seems have enjoyed better generalizability.\n",
    "\n",
    "The results are expected since regularization reduces overfitting and this is manifested through the deviation between the training accuracy and the reclassification accuracy. We implemented Dropout layers in the fully connected layers of the CNN, this is common practice in the field, however it is to be noted that regularization was not crucially needed in our model so far. The model became more robust because it became more generalizeable.\n",
    "\n",
    "Let's see if we can improve accuracy by adding batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5500290-6cb2-4a03-ac26-6d6b8d315ada",
   "metadata": {},
   "source": [
    "## Batch Normalization with Early Training Termination Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680097df-ca59-45a1-8015-9741b9cd2722",
   "metadata": {},
   "source": [
    "Adding batch normalization and early stopping features to the pipeline and demonstrating their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "851f65df-4466-45da-9a32-5d237e821546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "IMG_SIZE = 128\n",
    "IMG_CHANNEL= 3  # color channel\n",
    "\n",
    "MLP_HIDDEN= 16  # Hidden layer size\n",
    "\n",
    "LAST_LINEAR_SIZE = 1024  # 1024 is arbitrary\n",
    "N_CLASSES = 6  # output layer size\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "\n",
    "class PyTorchCNN(torch.nn.Module):\n",
    "    def __init__(self, epochs=10, eta=0.001, batch_size=1000, tolerance=4, min_delta=0.005, seed=0):\n",
    "        super(PyTorchCNN, self).__init__()\n",
    "        self.random = np.random.RandomState(seed)  # shuffle mini batches\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.batch_size = batch_size  # size of training batch - 1 would not work\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.model = None\n",
    "        self.counter = 0\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def init_layers(self, _K):\n",
    "        import torch.nn as nn\n",
    "        self.model = nn.Sequential(\n",
    "            # Conv -> (?, IMG_SIZE, IMG_SIZE, MLP_HIDDEN)\n",
    "            # Pool -> (?, IMG_SIZE/2, IMG_SIZE/2, MLP_HIDDEN)\n",
    "            nn.Conv2d(IMG_CHANNEL, MLP_HIDDEN, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(MLP_HIDDEN),\n",
    "\n",
    "            nn.Conv2d(MLP_HIDDEN, MLP_HIDDEN*2, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(start_dim=1),\n",
    "            \n",
    "            nn.Dropout(0.5, inplace=True),\n",
    "            nn.Linear(MLP_HIDDEN*2 * 29*29, LAST_LINEAR_SIZE), #27k to 10k\n",
    "            nn.BatchNorm1d(LAST_LINEAR_SIZE),\n",
    "            nn.Dropout(0.5, inplace=True),\n",
    "            nn.Linear(LAST_LINEAR_SIZE, _K) #1k to 6\n",
    "        )\n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()\n",
    "\n",
    "    def predict(self, _X):\n",
    "        _X = torch.FloatTensor(_X)\n",
    "        assert self.model is not None\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(_X)\n",
    "        ypred = torch.argmax(logits, dim=1)\n",
    "        return ypred.cpu().numpy()\n",
    "\n",
    "    def fit(self, _train_dl, _valid_dl, info=False):\n",
    "        import sys\n",
    "        import torch.nn as nn\n",
    "        self.init_layers(N_CLASSES)\n",
    "\n",
    "        optimizer = torch.optim.Rprop(self.model.parameters(), lr=self.eta) #this optimizer does not support regularization, instead it uses resilient back-propagation.\n",
    "        \n",
    "        # to track the training loss as the model trains\n",
    "        train_losses = []\n",
    "        # to track the validation loss as the model trains\n",
    "        valid_losses = []\n",
    "\n",
    "        # initialize the early_stopping object\n",
    "        # The main training loop\n",
    "        for e in range(self.epochs):\n",
    "            for data in _train_dl:\n",
    "                X, y = data[0], data[1]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                net_out = self.model(X)\n",
    "                loss = self.loss_func(net_out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_losses.append(loss.item())            \n",
    "            if info:\n",
    "                with torch.no_grad():\n",
    "                    acc = accuracy_score(data[1].tolist(), np.argmax(self.model(X).cpu(), axis=1).tolist())\n",
    "                    epoch_train_loss = np.average(train_losses)\n",
    "                    for vdata in _valid_dl:\n",
    "                        X_val, y_val = vdata[0], vdata[1]\n",
    "                        output = self.model(X_val)\n",
    "                        # calculate the loss\n",
    "                        val_loss = self.loss_func(output, y_val)\n",
    "                    \n",
    "                    # record validation loss\n",
    "                    valid_losses.append(val_loss.item())\n",
    "                    val_acc = accuracy_score(vdata[1].tolist(), np.argmax(self.model(X_val).cpu(), axis=1).tolist())\n",
    "\n",
    "                print(f\"\\r{e+1:02d}/{self.epochs:02d} | Loss: {loss:<6.2f} | Train/Valid Acc.: {acc*100:.2f}%/{val_acc*100:.2f}%\")\n",
    "                print('epoch_validate_loss',valid_losses[-1])\n",
    "                print('epoch_train_loss',train_losses[-1])\n",
    "                if (valid_losses[-1] - train_losses[-1]) > self.min_delta:\n",
    "                    self.counter +=1\n",
    "                    print(self.counter)\n",
    "                    if self.counter >= self.tolerance:  \n",
    "                        print(f'training terminated at epoch {e+1}')\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3efd82c9-838d-45e7-816c-efbb08803bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchCNN(\n",
      "  (loss_func): CrossEntropyLoss()\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Dropout(p=0.5, inplace=True)\n",
      "    (9): Linear(in_features=26912, out_features=1024, bias=True)\n",
      "    (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=True)\n",
      "    (12): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = PyTorchCNN(epochs=10, eta=0.001, batch_size=BATCH_SIZE)\n",
    "\n",
    "# example - the fit function will override the NN configuration\n",
    "cnn.init_layers(N_CLASSES)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91790a12-ebe1-4559-b33f-4ebe506e6c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/10 | Loss: 2.04   | Train/Valid Acc.: 52.69%/49.54%\n",
      "epoch_validate_loss 1.8442014455795288\n",
      "epoch_train_loss 2.038452386856079\n",
      "02/10 | Loss: 1.32   | Train/Valid Acc.: 63.36%/61.40%\n",
      "epoch_validate_loss 1.2950190305709839\n",
      "epoch_train_loss 1.3248707056045532\n",
      "03/10 | Loss: 1.17   | Train/Valid Acc.: 64.98%/65.11%\n",
      "epoch_validate_loss 1.0313133001327515\n",
      "epoch_train_loss 1.1695287227630615\n",
      "04/10 | Loss: 0.92   | Train/Valid Acc.: 71.09%/68.64%\n",
      "epoch_validate_loss 0.9020087122917175\n",
      "epoch_train_loss 0.9207519888877869\n",
      "05/10 | Loss: 0.76   | Train/Valid Acc.: 73.45%/69.17%\n",
      "epoch_validate_loss 0.853967010974884\n",
      "epoch_train_loss 0.7645508050918579\n",
      "1\n",
      "06/10 | Loss: 0.74   | Train/Valid Acc.: 73.37%/70.03%\n",
      "epoch_validate_loss 0.8035445809364319\n",
      "epoch_train_loss 0.7399264574050903\n",
      "2\n",
      "07/10 | Loss: 0.69   | Train/Valid Acc.: 76.55%/71.77%\n",
      "epoch_validate_loss 0.7694196105003357\n",
      "epoch_train_loss 0.6877537965774536\n",
      "3\n",
      "08/10 | Loss: 0.76   | Train/Valid Acc.: 74.59%/72.13%\n",
      "epoch_validate_loss 0.7665557861328125\n",
      "epoch_train_loss 0.7626846432685852\n",
      "09/10 | Loss: 0.70   | Train/Valid Acc.: 75.33%/72.99%\n",
      "epoch_validate_loss 0.7503593564033508\n",
      "epoch_train_loss 0.6973611116409302\n",
      "4\n",
      "training terminated at epoch 9\n",
      "CPU times: user 39min 36s, sys: 9min 15s, total: 48min 52s\n",
      "Wall time: 12min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn.fit(train_loader, validation_loader, info=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76757fac-028f-4c5b-9333-74fb512fe773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7383333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test performance\n",
    "y_test, y_pred = [], []\n",
    "for data in test_loader:\n",
    "    X = data[0]\n",
    "    y_test += data[1].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred += cnn.predict(X).tolist()\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c98a7-225d-4205-b6e1-0bc2a96d7c0d",
   "metadata": {},
   "source": [
    "As you can see batch normalization drastically improved performance in accuracy. Batch normalization takes outputs from layers and normalizes them before passing them on to other layers. This normalization allows the optimizer to work more effectively to a local optimum, and it improves the rate of training. Batch Normalization is required due to a phenomenon called internal covariate shift, that is, change in the distribution of layer inputs due to updates in the previous layer; that continous change negatively impacts learning. So batch normalization mellows out the change by applying normalization. For our case batch normalization improved the accuracy, which means that our optimizer's convergence was more accurate.\n",
    "\n",
    "Now in terms of early termination, we created an algorithm that works by comparing validation loss to training loss, and measures if the validation loss is greater than the training loss by a significant amount named min_delta, in this situation, further training would not be needed. So we provide a tolerance value which is a threshold count, when this threshold count is met then training terminates. For this example we set threshold to be 4, and it saved us an epoch of training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
